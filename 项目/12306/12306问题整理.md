# 为什么做这个项目（这个项目哪来的）
在校招前，想准备一个简历上项目，因为谷粒商城和瑞吉外卖已经被大家用的比较多了，所以我就没太考虑用这两个项目。我在 GitHub 和 Gitee 中找了找类似的开源项目，找到了一个很符合我期望的项目：12306。
因为 12306 是一个大规模的在线火车票订购平台，它涉及到许多复杂的业务逻辑和高并发处理，而且知名度也比较高，所以我觉得参与这个项目，我可以学到很多有关软件开发和系统设计方面的知识和技能。
而且，这个项目拥有比较丰富的前后端项目，架构分为 SpringBoot 单体版本和 SpringCloud 聚合版本，整体比较完善。
为了完成这个项目，我首先深入学习了 SpringBoot 和 SpringCloud Alibaba 框架，以及与数据库交互的知识。
随后，我对 12306 系统进行了需求分析，包括用户的登录、车票的查询、订单的生成等功能。我在开发过程中遇到了一些挑战，比如如何处理并发访问和保证数据的一致性、RocketMQ 消息防止重复消费等等问题。但通过不断地学习和尝试，我逐渐解决了这些问题，并完成了一个基本功能完善的 12306 项目。
我觉得通过参与这个项目，不仅学到了很多技术知识，还锻炼了解决问题的能力和较多的代码设计能力。这个项目也让我更加了解了软件开发的流程和规范，以及如何在一个复杂的系统中做出合理的设计和优化。
# 列车如何完成数据检索
首先要说明采用的是redis完成数据检索
采用hash数据结构：
key为region_train_station_起始站_终点站_日期
value的key为车站id_起始站_终点站，value为列车详细信息
V1版本：
1.责任链模式验证数据是否正确
2.通过起始、终点、日期等信息构建key查询缓存获取列车的详细信息，并自定义时间比较器对列车按出发时间进行排序。由于余票是实时更新的，所以这里的缓存中只有列车的基本信息
3.余票信息在另外的key下，要继续通过上面查到的列车id+起始终点站信息构建key获取余票信息
4.最后构建响应数据返回
V2版本：
V1版本需要频繁的查询缓存，可能存在严重的性能深渊问题
1. 分布式锁的使用和缓存逻辑
2. 数据库操作频繁
3. Redis的频繁操作
4. 列车实体集合可能很大
v2进行了优化：
1.使用定时任务加载缓存，确保缓存中有值，避免了分布式锁和双重校验锁的使用
2.通过redis管道优化余票查询和价格查询，减少了redis的连接，加快了响应速度
# 为什么采用Redis
主要有几点考量：
**实时性能**：Redis 以内存为基础，具有极低的读取延迟，可以快速响应实时查询请求，这对于需要即时更新的列车数据非常重要。单程或往返、出发日期等条件可以通过快速的 Redis 查询来满足。
**并发性能**：支持多个客户端**并发连接**，每个客户端可以独立地执行读取和写入操作，这使得它在处理高并发请求时表现出色。Redis 提供了**原子性操作**，如原子增减和原子加锁，这些操作可以在多线程或多进程环境下安全使用，有助于处理并发操作。
**部署成本**：轻量级并且可以通过集群方式简单地扩展
**简单性**：Redis 的数据模型相对简单，适合存储简单的键值对或一些常规数据结构。这使得 Redis 适合存储一些搜索条件，如出发地、目的地、车次类型等，以便快速筛选结果。
在实现上，并不是将所有的搜索条件都提交给后端，而是前端经过简单的筛选后，只保留出发、终点、日期再提交给后端进行查询，前端通过列车详细信息中的其他信息选择是否展示
# 注册用户如何防止缓存穿透
缓存穿透常规解决方案：
1.缓存null，该值过期时间内不可再次被使用，如果有大量并发请求查询不存在的用户名，可能会导致数据库短时间内被打挂。
2.布隆过滤器，用户可以注销，用户名可以重新使用，而布隆过滤器不能删除元素
3.Redis Set 存储已注册用户名，占用大量内存
4.分布式锁，影响性能
最终实现方案：
布隆过滤器+reuse缓存
1.查询布隆过滤器，如果不存在则可以直接使用，若存在则继续查询缓存（用户注销后将其用户名放入该缓存）
2.如果缓存中有，可直接使用；若没有，可认为该用户名已经被使用
误判问题：设置的是0.03误判率，并且对于业务来说一个用户名用不了影响不大
reuse缓存只进不出，可能占用较大内存：
1.限制单用户（按证件号）只能注销五次
2.所有用户名存在同一个key下，可能出现大key，分片处理，分为1024片，对用户名进行hashcode取模存储
# 关于线程池的使用
如果一个用户购买多种类型的车座，比如同一订单购买了一等座和二等座，那么就要按照不同的座位逻辑进行选座。这个逻辑是可以并行的，使用线程池。
1.线程池执行是异步的，有可能购票流程都结束了，这里还没执行完分配座位逻辑。需要拿到线程池执行的结果后再继续执行后续的提交订单逻辑。**通过Future获取执行的结果**。
2.参数：阻塞队列使用BlockingQueueTypeEnum.SYNCHRONOUS_QUEUE，一个没有容量的阻塞队列，没有容量自然也不会有堆积；如果线程池满了会触发拒绝策略，如果抛异常就违反了咱们设计的初衷。从 AbortPolicy 修改为 CallerRunsPolicy，代表如果触发拒绝策略，则由提交任务的线程运行该任务。
3.通过并行流收集future.get(List\<TrainPurchaseTicketRespDTO>)每种车票的每张车票

在使用并行流之前一定要考虑以下问题:
1. 业务场景是否真的需要并行处理? （需要，提升速度）
2. 并行处理任务是否是相对独立?（独立） 是否会引起并行间的竞态条件?（CopyOnWriteArrayList） 
3. 并行处理是否依赖任务的执行顺序?（不依赖）

# 如何防止超卖（如何应对高并发）
常见超卖解决方案：数据库乐观锁（更新数据库时判断余票是否充足），redis余量扣减
但是扣减余量时要扣减站点相关的多个站点库存，有关联属性。所以上面的这些版本并不适用于 12306 的列车座位余量扣减业务。
实际采用：令牌限流
在分布式锁前面加一个令牌桶，只有获取到令牌的请求才可以继续获取分布式锁进而完成订单的创建
主要做了三件事：
1. 如果令牌容器在缓存中失效需要重新读取并放入缓存；
2. 准备执行 Lua 脚本的数据；
3. 最终的执行 Lua 脚本获取令牌。
令牌桶hash结构，value是 key为‘北京南_南京南_0’,value是余票值的 hash结构
lua脚本参数：
- actualHashKey：存储 Redis Hash 结构的 Key 值（令牌桶的key）
- luaScriptKey：用户购买的出发站点和到达站点，比如北京南_南京南。
- seatTypeCountArray：需要扣减的座位类型以及对应数量。
- takeoutRouteDTOList：需要扣减的相关列车站点。
在lua脚本中完成多站点的余票扣减逻辑
# 中间站点余票的更新逻辑
列车座位状态枚举如下所示，共有三个状态：可售、锁定、已售。
余票扣减伴随着两个逻辑：
1. 一个是锁定数据库的列车座位车票状态记录，从可售状态变更为锁定状态。
2. 另一个是将缓存中的座位余量进行扣减，卖出去一个自减一，卖出去两个自减二。
这里涉及到缓存与数据库的一致性问题
# 缓存与数据库的一致性问题
可以根据业务场景选择下述缓存一致性方案：
- 缓存双删:如果公司**现有消息队列中间件**，可以考虑使用该方案，反之则不需要考虑。
- 先写数据库再删缓存：这种方案从**实时性以及技术实现复杂度**来说都比较不错，推荐大家使用这种方案。
- Binlog 异步更新缓存：如果希望实现**最终一致性**以及数据多中心模式，该方案无疑是最合适的。
采用了一种确保缓存和数据库一致性的方案，使用 Canal 监听 Binlog 模式。该方案将数据库的数据变更通过 Canal 转发给消息队列的特定 Topic。客户端应用程序可以监听该消息队列的 Topic，以保持缓存与数据库的一致性。
# 余票binlog更新延迟问题
不是问题，展示的延迟与超卖没有关系
那为什么要分开来，不直接用令牌数量作为余票数量：
为了增加并发量。 余票用来展示、查票用，令牌桶专门用来买票。
令牌桶的结构就是一个列车id一个令牌桶，余票是列车id+出发站+目的站，有人经常刷新找有没有车票的话只需查询对应两个车站的余票，单用令牌桶锁的粒度太大，性能不够
# 监听Binlog的RocketMQ如何保证顺序性
1.消息组
RocketMQ 顺序消息的顺序关系通过消息组（MessageGroup）判定和识别，发送顺序消息时需要为每条消息设置归属的消息组，相同消息组的多条消息之间遵循先进先出的顺序关系，不同消息组、无消息组的消息之间不涉及顺序性。
2.生产顺序性
RocketMQ 通过生产者和服务端的协议保障单个生产者串行地发送消息，并按序存储和持久化。如需保证消息生产的顺序性，则必须满足以下条件：
- 单一生产者：消息生产的顺序性仅支持单一生产者，不同生产者分布在不同的系统，即使设置相同的消息组，不同生产者之间产生的消息也无法判定其先后顺序。
- 串行发送：Apache RocketMQ 生产者客户端支持多线程安全访问，但如果生产者使用多线程并行发送，则不同线程间产生的消息将无法判定其先后顺序。
- 满足以上条件的生产者，将顺序消息发送至RocketMQ 后，会保证设置了同一消息组的消息，按照发送顺序存储在同一队列中。相同消息组的消息按照先后顺序被存储在同一个队列。不同消息组的消息可以混合在同一个队列中，且不保证连续。
3.消费顺序性
RocketMQ 通过消费者和服务端的协议保障消息消费严格按照存储的先后顺序来处理。如需保证消息消费的顺序性，则必须满足以下条件：
- 投递顺序：RocketMQ 通过客户端SDK和服务端通信协议保障消息按照服务端存储顺序投递，但业务方消费消息时需要严格按照接收---处理---应答的语义处理消息，避免因异步处理导致消息乱序。备注消费者类型为PushConsumer时， Apache RocketMQ 保证消息按照存储顺序一条一条投递给消费者，若消费者类型为SimpleConsumer，则消费者有可能一次拉取多条消息。此时，消息消费的顺序性需要由业务方自行保证。
- 有限重试：RocketMQ 顺序消息投递仅在重试次数限定范围内，即一条消息如果一直重试失败，超过最大重试次数后将不再重试，跳过这条消息消费，不会一直阻塞后续消息处理。对于需要严格保证消费顺序的场景，请务必设置合理的重试次数，避免参数不合理导致消息乱序。

# 延期关闭订单，除了用 RocketMQ 还有什么方案？各自优缺点？
定时任务
 **RocketMQ** 
1. **异步处理：** 使用 RocketMQ 可以将订单关闭的任务异步发送到消息队列，不阻塞主业务流程，提高系统的响应速度。
2. **解耦性：** 通过消息队列，订单关闭的逻辑和主业务逻辑解耦，提高系统的可维护性和可扩展性。
3. **分布式支持：** 如果系统是分布式的，RocketMQ天生支持分布式部署，适用于大规模系统。
**缺点：**
1. **引入消息中间件：** 使用 RocketMQ 需要引入消息中间件，增加了系统的复杂性和维护成本。
2. **消息可能丢失：** 尽管 RocketMQ 提供高可靠性，但在极端情况下，消息可能会丢失。
 
 **定时任务方案：**
1. **简单直接：** 定时任务是一种比较简单直接的方案，不需要引入消息中间件，减少了系统的复杂性。
2. **稳定性：** 定时任务通常由调度器触发，相对稳定可靠，不容易丢失任务。
**缺点：**
1. **同步处理：** 定时任务通常是同步执行的，可能会阻塞主业务流程，影响系统的响应速度。
2. **不适合分布式：** 如果系统是分布式的，定时任务可能需要一些分布式调度的解决方案，增加了一些复杂性。
3. **难以维护：** 随着定时任务的增多，可能会难以维护和监控。

**如何选择：**
1. **系统架构：** 如果系统已经使用消息中间件，且具备分布式特性，RocketMQ 方案可能更适合。如果系统相对简单，并且不需要引入消息中间件，定时任务方案可能更合适。
2. **实时性要求：** 如果对订单关闭的实时性要求不高，且可以异步处理，RocketMQ 是一个不错的选择。如果需要及时处理订单关闭，定时任务可能更合适。
3. **可维护性：** 如果系统对可维护性有更高要求，RocketMQ 可以提供更好的解耦性和可维护性。如果追求简单直接，定时任务可能更符合需求。
# 基因法分库分表
订单分库分表的基本查询条件是用户要能查看自己的订单，另外，也要支持订单号精准查询。
这样的话，我们就需要按照两个字段当做分片键，这也就意味着每次查询时需要带着用户和订单两个字段，非常的不方便。能不能通过一个字段分库分表，但是查询时两个字段任意传一个就能精准查询，而不导致读扩散问题？
生成订单号时将用户id的后六位冗余到订单号中，并按照用户id后六位进行分库分表，将分片键定义为用户id和订单号，只要查询中携带这两个字段，我们就取用户 ID 后六位进行查找分片表的位置（用户id后六位和订单号后六位是一样的）。




1. 使用redis而非es进行**列车数据检索**，实现检索效率的提升
2. 优化**布隆过滤器+reuse缓存池**实现用户注册与注销，实现用户名复用，解决**缓存穿透**
3. 使用**令牌限流算法**应对海量用户购票请求，实现服务高可用和**解决超卖**问题
4. 使用**基因法分库分表**实现同一个分片键支持订单号和用户ID两个查询维度
5. 基于binlog+rocketMQ**异步更新缓存**实现缓存与数据库间的最终一致性
6. 使用**定时任务**更新缓存+redis管道优化减少分布式锁和双重校验锁的使用
7. 自定义注解+AOP实现RocketMQ消息**消费幂等性**

1. 使用**策略模式+模板模式**设计消息类型及处理器，实现代码复用和可扩展性
2. 通过更新数据库删除缓存策略，保障缓存与数据库之间的**数据一致性**功能
3. 基于**流**完成文件分块、组装和传输，上传下载效率提升
4. 封装缓存组件库避免注册用户时，用户名全局唯一带来的**缓存穿透**问题，减轻数据库访问压力
5. 使用**线程池**开辟多线程，提升文件块传输效率
6. 通过**布隆过滤器**完成判断文件是否已存在，性能远胜分布式锁查询数据库方案
7. 通过redis+**自定义雪花算法**实现分布式情况下文件id全局唯一